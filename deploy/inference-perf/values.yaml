# inference-perf/values.yaml
job:
  image:
    repository: quay.io/inference-perf/inference-perf
    tag: "" # Defaults to .Chart.AppVersion
  nodeSelector: {}
  # Example resources:
  # resources:
  #   requests:
  #     cpu: "1"
  #     memory: "4Gi"
  #   limits:
  #     cpu: "2"
  #     memory: "8Gi"
  resources: {}

logLevel: INFO

# hfToken optionally creates a secret with the specified token.
# Can be set using helm install --set hftoken=<token>
hfToken: ""

config:
  load:
    type: constant
    stages:
    - rate: 1
      duration: 30
  api:
    type: completion
  server:
    type: vllm
    base_url: http://0.0.0.0:8000
    ignore_eos: true
  data:
    type: shareGPT
  metrics:
    type: prometheus
    prometheus:
      url: http://localhost:9090
      scrape_interval: 15
  report:
    request_lifecycle:
      summary: true
      per_stage: true
      per_request: false
    prometheus:
      summary: true
      per_stage: false
